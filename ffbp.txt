#ffbp neural 
from math import exp

def activate(weights, inputs):
    activation = weights[-1]
    for i in range(len(weights)-1):
        activation += weights[i] * inputs[i]
    return activation

def transfer(activation):
    return 1.0 / (1.0 + exp(-activation))

def forward_propagate(network, row):
    inputs = row
    for layer in network:
        new_inputs = []
        for neuron in layer:
            activation = activate(neuron['weights'], inputs)
            neuron['output'] = transfer(activation)
            new_inputs.append(neuron['output'])
        inputs = new_inputs
    return inputs

network = [
    [{'weights':[0.13, 0.84]}],
    [{'weights':[0.25, 0.49]}],
    [{'weights':[0.44, 0.65]}]
]
row = [1, 0, None]
output = forward_propagate(network, row)
print('Output:', output)